---

- name: get Hadoop distro (except CDH4)
  become: no
  local_action: 'get_url url={{ hadoop_download_url }} dest=files/'
  when: "hadoop_version != 'cdh4'"

- name: get Hadoop distro (CDH4)
  become: no
  local_action: get_url url={{ hadoop_download_url }} dest=files/
  when: "hadoop_version == 'cdh4'"


- name: distribute hadoop among slaves
  synchronize:
    src: "files/{{ hadoop_file }}.tar.gz"
    dest: "/usr/local/{{ hadoop_file }}.tar.gz"
    checksum: yes
#  delegate_to: "{{ active_master_inventory_hostname }}"

- name: unzip hadoop
  unarchive: copy=no src=/usr/local/{{ hadoop_file }}.tar.gz dest=/usr/local/ owner={{ hadoop_user }} group=hadoop

- name: create hadoop symlink
  file: src=/usr/local/{{ hadoop_file }} dest=/usr/local/hadoop state=link


- name: put needed swift library
  get_url: url={{ swift_download_url }} dest=/usr/local/{{ hadoop_file }}/share/hadoop/hdfs/lib/
  when: "hadoop_version == 2.3 or hadoop_version == 2.6"

#- name: remove hadoop archive
#  file: path=/usr/local/{{ hadoop_file }}.tar.gz state=absent

- name: set user and priviliges on hadoop
  file: path=/usr/local/{{ hadoop_file }} owner={{ hadoop_user }} group=hadoop recurse=yes

- name: get Spark distro
  become: no
  local_action: get_url url={{ spark_download_url }} dest=files/

#- name: put spark distro to master
#  copy: src={{ spark_arch }} dest=/opt/
#  when: "'{{ inventory_hostname }}' == '{{ active_master_inventory_hostname }}'"

- name: distribute spark distro among slaves
  synchronize:
    src: "files/{{ spark_file }}.tgz"
    dest: "/opt/{{ spark_file }}.tgz"
    checksum: no
#  delegate_to: "{{ active_master_inventory_hostname }}"

- name: unzip spark
  unarchive: copy=no src=/opt/{{ spark_file }}.tgz dest=/opt

#- name: remove spark archive
#  file: path=/opt/{{ spark_file }}.tgz state=absent

- name: create spark symlink
  file: src={{ spark_home }} dest=/opt/spark state=link

- name: create spark symlink
  file: src={{ spark_home }} dest=/usr/local/spark state=link


- name: create extra jars directory
  file: path={{ spark_extra_jars_dir }} owner={{ hadoop_user }} group=hadoop state=directory

- name: copy extra jars
  copy: src={{item.path}} dest={{ spark_extra_jars_dir }}/{{item.name}} owner={{ hadoop_user }} group=hadoop mode=0644
  with_items: "{{extra_jars}}"

