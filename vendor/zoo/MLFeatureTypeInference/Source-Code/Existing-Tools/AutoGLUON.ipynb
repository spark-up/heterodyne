{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copyright 2020 Vraj Shah, Arun Kumar\n",
    "#\n",
    "#Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#you may not use this file except in compliance with the License.\n",
    "#You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#Unless required by applicable law or agreed to in writing, software\n",
    "#distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#See the License for the specific language governing permissions and\n",
    "#limitations under the License.\n",
    "\n",
    "import autogluon as ag\n",
    "from autogluon import TabularPrediction as task\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = pd.read_csv('../data_test.csv')\n",
    "test_metadata = pd.read_csv('../RawCSV/Metadata/meta_data.csv')\n",
    "\n",
    "print(len(testdf),len(test_metadata))\n",
    "test_merged = pd.merge(testdf,test_metadata,on='Record_id')\n",
    "print(len(test_merged))\n",
    "\n",
    "print(test_merged)\n",
    "y_true = test_merged.y_act.values.tolist()\n",
    "print(y_true)\n",
    "print(len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first change the AutoGluon's default_learner's fit function to make sure that we just return the ML feature types rather than building their entire ML pipeline.\n",
    "# Just replace the fit function with this codeblock in project_env/lib/python3.7/site-packages/autogluon/utils/tabular/ml/learner/default_learner.py\n",
    "\n",
    "# def fit(self, X: DataFrame, X_test: DataFrame = None, scheduler_options=None, hyperparameter_tune=True,\n",
    "#         feature_prune=False, holdout_frac=0.1, num_bagging_folds=0, num_bagging_sets=1, stack_ensemble_levels=0,\n",
    "#         hyperparameters=None, time_limit=None, save_data=False, save_bagged_folds=True, verbosity=2):\n",
    "#     \"\"\" Arguments:\n",
    "#             X (DataFrame): training data\n",
    "#             X_test (DataFrame): data used for hyperparameter tuning. Note: final model may be trained using this data as well as training data\n",
    "#             hyperparameter_tune (bool): whether to tune hyperparameters or simply use default values\n",
    "#             feature_prune (bool): whether to perform feature selection\n",
    "#             scheduler_options (tuple: (search_strategy, dict): Options for scheduler\n",
    "#             holdout_frac (float): Fraction of data to hold out for evaluating validation performance (ignored if X_test != None, ignored if kfolds != 0)\n",
    "#             num_bagging_folds (int): kfolds used for bagging of models, roughly increases model training time by a factor of k (0: disabled)\n",
    "#             num_bagging_sets (int): number of repeats of kfold bagging to perform (values must be >= 1),\n",
    "#                 total number of models trained during bagging = num_bagging_folds * num_bagging_sets\n",
    "#             stack_ensemble_levels : (int) Number of stacking levels to use in ensemble stacking. Roughly increases model training time by factor of stack_levels+1 (0: disabled)\n",
    "#                 Default is 0 (disabled). Use values between 1-3 to improve model quality.\n",
    "#                 Ignored unless kfolds is also set >= 2\n",
    "#             hyperparameters (dict): keys = hyperparameters + search-spaces for each type of model we should train.\n",
    "#     \"\"\"\n",
    "#     if hyperparameters is None:\n",
    "#         hyperparameters = {'NN': {}, 'GBM': {}}\n",
    "#     # TODO: if provided, feature_types in X, X_test are ignored right now, need to pass to Learner/trainer and update this documentation.\n",
    "#     if time_limit:\n",
    "#         self.time_limit = time_limit\n",
    "#         logger.log(20, f'Beginning AutoGluon training ... Time limit = {time_limit}s')\n",
    "#     else:\n",
    "#         self.time_limit = 1e7\n",
    "#         logger.log(20, 'Beginning AutoGluon training ...')\n",
    "#     logger.log(20, f'AutoGluon will save models to {self.path}')\n",
    "#     logger.log(20, f'Train Data Rows:    {len(X)}')\n",
    "#     logger.log(20, f'Train Data Columns: {len(X.columns)}')\n",
    "#     if X_test is not None:\n",
    "#         logger.log(20, f'Tuning Data Rows:    {len(X_test)}')\n",
    "#         logger.log(20, f'Tuning Data Columns: {len(X_test.columns)}')\n",
    "#     time_preprocessing_start = time.time()\n",
    "#     logger.log(20, 'Preprocessing data ...')\n",
    "\n",
    "#     X_before=X\n",
    "#     X, y, X_test, y_test, holdout_frac, num_bagging_folds = self.general_data_processing(X, X_test, holdout_frac, num_bagging_folds)\n",
    "#     time_preprocessing_end = time.time()\n",
    "#     self.time_fit_preprocessing = time_preprocessing_end - time_preprocessing_start\n",
    "#     logger.log(20, f'\\tData preprocessing and feature engineering runtime = {round(self.time_fit_preprocessing, 2)}s ...')\n",
    "#     if time_limit:\n",
    "#         time_limit_trainer = time_limit - self.time_fit_preprocessing\n",
    "#     else:\n",
    "#         time_limit_trainer = None\n",
    "\n",
    "#     df = pd.DataFrame(columns=['column', 'feature_type'])\n",
    "#     # Imported from utils/tabular/features/abstract_feature_generator\n",
    "#     def check_if_nlp_feature(X):\n",
    "#         import re\n",
    "#         X_unique = X.unique()\n",
    "#         num_unique = len(X_unique)\n",
    "#         num_rows = len(X)\n",
    "#         unique_ratio = num_unique / num_rows\n",
    "#         if unique_ratio <= 0.01: return False\n",
    "#         avg_words = np.mean([len(re.sub(' +', ' ', value).split(' ')) if isinstance(value, str) else 0 for value in X_unique])\n",
    "#         if avg_words < 3: return False\n",
    "#         return True\n",
    "\n",
    "#     def check_mark_for_removal(X):\n",
    "#         col_val = X\n",
    "#         num_unique = len(col_val.unique())\n",
    "#         mark_for_removal = False\n",
    "#         if num_unique == 1: mark_for_removal = True    \n",
    "#         return mark_for_removal\n",
    "\n",
    "#     def check_if_datetime_feature(X):\n",
    "#         try:\n",
    "#             X.apply(pd.to_datetime)\n",
    "#             return True\n",
    "#         except: return False\n",
    "\n",
    "#     def check_numeric_dtypes(X):\n",
    "#         if X.dtypes == 'int64' or X.dtypes == 'float64': return True\n",
    "#         return False\n",
    "\n",
    "#     def check_unusable_for_categ(X):\n",
    "#         rank = X.value_counts().sort_values(ascending=True)\n",
    "#         rank = rank[rank >= 3]\n",
    "#         rank = rank.reset_index()\n",
    "#         val_list = list(rank['index'].values)\n",
    "#         if len(val_list) <= 1: return True\n",
    "#         return False\n",
    "\n",
    "#     print(X_before)\n",
    "#     for col in X_before.columns:\n",
    "#         if col=='label_target': continue\n",
    "#         df_col = X_before[col]\n",
    "#         numeric = check_numeric_dtypes(df_col)\n",
    "#         text = check_if_nlp_feature(df_col)\n",
    "#         unusable = check_mark_for_removal(df_col)\n",
    "#         date = check_if_datetime_feature(df_col)\n",
    "\n",
    "#         curpred=0\n",
    "#         if unusable: curpred = 7\n",
    "#         elif text: curpred = 3\n",
    "#         elif date: curpred = 2\n",
    "#         if date and numeric and (not unusable): curpred = 0\n",
    "\n",
    "#         if (not numeric) and curpred == 0: \n",
    "#             if check_unusable_for_categ(df_col): curpred = 7\n",
    "#             else: curpred = 1\n",
    "\n",
    "#         df.loc[len(df)] = [col, curpred]\n",
    "\n",
    "#     df.to_csv(\"AutoGluon_predictions.csv\",index=False, mode='a', header=False)\n",
    "#     return self.feature_generator.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Load_GLUON(dataDownstream):\n",
    "    df = pd.DataFrame(columns=['column', 'feature_type'])\n",
    "    df.to_csv('AutoGluon_predictions.csv',index=False)\n",
    "\n",
    "    import time\n",
    "    time.sleep(3)\n",
    "    train= copy.deepcopy(dataDownstream)\n",
    "\n",
    "    train['label_target'] = 1\n",
    "    train_data = task.Dataset(df=train)\n",
    "    label_column = 'label_target'\n",
    "\n",
    "    try: features = task.fit(train_data=train_data, label=label_column)\n",
    "    except: AlwaysTrue = 1\n",
    "\n",
    "    agl_predictions = pd.read_csv('AutoGluon_predictions.csv')\n",
    "    predictions = agl_predictions['feature_type'].values.tolist()\n",
    "    \n",
    "    return predictions[0]\n",
    "    \n",
    "    \n",
    "cntExceptions = 0\n",
    "y_agl = [0]*1985\n",
    "prv_csv_name,csv_name = '',''\n",
    "exception_indices = []\n",
    "\n",
    "for index, row in test_merged.iterrows():\n",
    "    if index%100==0:  print(index)\n",
    "    col = row['Attribute_name']\n",
    "    prv_csv_name = csv_name\n",
    "    csv_name = '../../RawCSV/RawCSVFiles/' + row['name']\n",
    "    \n",
    "    if prv_csv_name != csv_name:  df = pd.read_csv(csv_name,encoding='latin1')\n",
    "\n",
    "    try:\n",
    "        df_col = df[[col]]\n",
    "    except KeyError:\n",
    "        y_agl[index]=1\n",
    "        exception_indices.append(row)\n",
    "        cntExceptions += 1\n",
    "        continue\n",
    "        \n",
    "    y_agl[index] = Load_GLUON(df_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "dict_label_true = {\n",
    "    'numeric': 0,\n",
    "    'categorical': 1,\n",
    "    'datetime': 2,\n",
    "    'sentence': 3,\n",
    "    'url': 4,\n",
    "    'embedded-number': 5,\n",
    "    'list': 6,\n",
    "    'not-generalizable': 7,\n",
    "    'context-specific': 8\n",
    "}\n",
    "\n",
    "y_true = [dict_label_true[str(i)] for i in y_true]\n",
    "\n",
    "print(accuracy_score(y_true, y_agl))\n",
    "print(confusion_matrix(y_true, y_agl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
