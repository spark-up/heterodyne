{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// #Copyright 2020 Vraj Shah, Arun Kumar\n",
    "// #\n",
    "// #Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "// #you may not use this file except in compliance with the License.\n",
    "// #You may obtain a copy of the License at\n",
    "// #\n",
    "// #    http://www.apache.org/licenses/LICENSE-2.0\n",
    "// #\n",
    "// #Unless required by applicable law or agreed to in writing, software\n",
    "// #distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "// #WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "// #See the License for the specific language governing permissions and\n",
    "// #limitations under the License."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Install TransmogrifAI library using\n",
    "https://docs.transmogrif.ai/en/stable/installation/index.html\n",
    "https://docs.transmogrif.ai/en/stable/examples/Running-from-Jupyter-Notebook.html\n",
    "\n",
    "Install both python and scala kernels for jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%classpath add mvn com.salesforce.transmogrifai transmogrifai-core_2.11 0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%classpath add mvn org.apache.spark spark-mllib_2.11 2.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "import com.salesforce.op._\n",
    "import com.salesforce.op.features._\n",
    "import com.salesforce.op.features.types._\n",
    "import com.salesforce.op.evaluators.Evaluators\n",
    "import java.io._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.salesforce.op.OpWorkflow\n",
    "import com.salesforce.op.evaluators.Evaluators\n",
    "import com.salesforce.op.readers.DataReaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// package com.salesforce.op.readers\n",
    "\n",
    "import com.salesforce.op.test.PassengerSparkFixtureTest\n",
    "import org.apache.avro.Schema\n",
    "import org.apache.avro.generic.GenericRecord\n",
    "import org.junit.runner.RunWith\n",
    "import org.apache.avro.generic.GenericRecord\n",
    "// import org.scalatest.FlatSpec\n",
    "// import org.scalatest.junit.JUnitRunner\n",
    "import scala.collection.JavaConverters._\n",
    "import java.nio.file.Paths\n",
    "import java.io.File\n",
    "import org.apache.spark.sql.execution.datasources.csv.CSVSchemaUtils\n",
    "import com.salesforce.op.utils.io.csv.{CSVInOut, CSVOptions, CSVToAvro}\n",
    "import com.salesforce.op.readers.{CSVDefaults}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val conf = new SparkConf().setMaster(\"local[*]\").setAppName(\"TestPrediction\")\n",
    "val conf = new SparkConf().setMaster(\"local[*]\").setAppName(\"TestPrediction\").set(\"spark.testing.memory\", \"2147480000\") //use this if memory limit not set\n",
    "\n",
    "\n",
    "// conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \n",
    "// conf.registerKryoClasses(Array(classOf[org.apache.avro.generic.GenericData$Record]))\n",
    "\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "conf.registerKryoClasses(Array(classOf[org.apache.avro.generic.GenericData.Record]))\n",
    "\n",
    "implicit val spark = SparkSession.builder.config(conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Python Snippet. Switch to Python Kernel\n",
    "import pandas as pd\n",
    "testdf = pd.read_csv('../../Benchmark-Labeled-Data/data_test.csv')\n",
    "test_metadata = pd.read_csv('../../RawCSV/Metadata/meta_data.csv')\n",
    "\n",
    "# print(len(testdf),len(test_metadata))\n",
    "test_merged = pd.merge(testdf,test_metadata,on='Record_id')\n",
    "test_merged\n",
    "\n",
    "prv_csv_name,csv_name = '',''\n",
    "for index, row in test_merged.iterrows():\n",
    "    if index%100==0:  print(index)\n",
    "    col = row['Attribute_name']\n",
    "    prv_csv_name = csv_name\n",
    "    csv_name = '../../RawCSV/RawCSVFiles/' + row['name']\n",
    "    \n",
    "    print(csv_name)\n",
    "    print(col)\n",
    "    \n",
    "    if prv_csv_name != csv_name:  df = pd.read_csv(csv_name,encoding='latin1')\n",
    "    \n",
    "    try: df_col = df[[col]]\n",
    "    except:\n",
    "        df = pd.read_csv(csv_name)\n",
    "        try: df_col = df[[col]]\n",
    "        except: df_col = pd.DataFrame(columns=[col])\n",
    "        \n",
    "    df_col.to_csv('column_level_csvs/'+ str(index) + '.csv',index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testDataDir: String = {\n",
    "Some(new File(\"column_level_csvs/\"))\n",
    "  .collect{ case d if d.isDirectory => d.getPath}\n",
    "  .getOrElse(Paths.get(\"column_level_csvs-sibling\").relativize(Paths.get(\"column_level_csvs\")).toString)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val fw = new FileWriter(\"TransmogrifAI_Predictions.csv\", true)\n",
    "\n",
    "for (i <- 0 until 1985) {\n",
    "print(i)\n",
    "\n",
    "\n",
    "val istr = i.toString\n",
    "// var Newstr = '../../' + '/' + istr + \".csv\"    \n",
    "var Newstr = testDataDir + '/' + istr + \".csv\"\n",
    "// def passengerCsvWithHeaderPath: String = Paths.get(Newstr, \"2.csv\").toString\n",
    "\n",
    "val finalPath = Newstr\n",
    "val options: CSVOptions = CSVDefaults.CSVOptions\n",
    "val csvData = new CSVInOut(options).readRDD(finalPath)\n",
    "val headers: Seq[String] = Seq.empty\n",
    "val hdrs = if (headers.nonEmpty) headers else csvData.first()\n",
    "val hdrsSet = hdrs.toSet\n",
    "val data = csvData.filter(_.exists(!hdrsSet.contains(_)))\n",
    "\n",
    "val columnPrunning = spark.sessionState.conf.csvColumnPruning\n",
    "\n",
    "val inferredSchema = CSVSchemaUtils.infer(data.map(_.toArray), hdrs, options, columnPrunning)\n",
    "\n",
    "fw.write(inferredSchema.toString)\n",
    "fw.write(\"\\n\")\n",
    "    \n",
    "println()\n",
    "println(inferredSchema)\n",
    "}\n",
    "\n",
    "fw.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Python Snippet. So, Again switch to Python Kernel\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "inf_types = pd.read_csv('TransmogrifAI_Predictions.csv',header=None)\n",
    "dict_label = {\n",
    "    'IntegerType': 0,\n",
    "    'DecimalType(4': 0,\n",
    "    'DoubleType':0, \n",
    "    'LongType':0, \n",
    "    'DecimalType(3': 0,\n",
    "    'StringType': 8,    \n",
    "    'TimestampType': 2,\n",
    "    'BooleanType': 8  \n",
    "}\n",
    "\n",
    "inf_types[1] = [dict_label[i] for i in inf_types[1]]\n",
    "y_trans = inf_types[1].values.tolist()\n",
    "print(len(y_trans))\n",
    "\n",
    "df = pd.read_csv('../../Benchmark-Labeled-Data/data_test.csv')\n",
    "\n",
    "dict_label_true = {\n",
    "    'numeric': 0,\n",
    "    'categorical': 1,\n",
    "    'datetime': 2,\n",
    "    'sentence': 3,\n",
    "    'url': 4,\n",
    "    'embedded-number': 5,\n",
    "    'list': 6,\n",
    "    'not-generalizable': 7,\n",
    "    'context-specific': 8\n",
    "}\n",
    "\n",
    "y_true = df.y_act.values.tolist()\n",
    "y_true = [dict_label_true[str(i)] for i in y_true]\n",
    "                 \n",
    "print(accuracy_score(y_true, y_trans))\n",
    "print(confusion_matrix(y_true, y_trans))                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
