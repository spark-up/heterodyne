{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Copyright 2020 Vraj Shah, Arun Kumar\n",
    "#\n",
    "#Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#you may not use this file except in compliance with the License.\n",
    "#You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#Unless required by applicable law or agreed to in writing, software\n",
    "#distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#See the License for the specific language governing permissions and\n",
    "#limitations under the License.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Input, Flatten, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate,GlobalMaxPooling1D\n",
    "from keras.preprocessing import text as keras_text, sequence as keras_seq\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "np.random.seed(512)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import LeakyReLU, BatchNormalization\n",
    "from keras import initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from itertools import chain\n",
    "\n",
    "# define network parameters\n",
    "max_features = 256\n",
    "maxlen = 256\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def func1(data,y):\n",
    "\n",
    "    data1 = data[['total_vals', 'num_nans', '%_nans', 'num_of_dist_val', '%_dist_val', 'mean',\n",
    "           'std_dev', 'min_val', 'max_val', 'mean_word_count', 'std_dev_word_count',\n",
    "            'mean_stopword_total', 'mean_whitespace_count',\n",
    "           'mean_char_count', 'mean_delim_count', 'stdev_stopword_total',\n",
    "           'stdev_whitespace_count', 'stdev_char_count', 'stdev_delim_count'\n",
    "           ]]\n",
    "\n",
    "    data1 = data1.reset_index(drop=True)\n",
    "    data1 = data1.fillna(0)\n",
    "\n",
    "    data1 = data1.rename(columns={\n",
    "        'mean': 'scaled_mean',\n",
    "        'std_dev': 'scaled_std_dev',\n",
    "        'min_val': 'scaled_min',\n",
    "        'max_val': 'scaled_max',        \n",
    "        'mean_word_count': 'scaled_mean_token_count',\n",
    "        'std_dev_word_count': 'scaled_std_dev_token_count',\n",
    "        '%_nans': 'scaled_perc_nans',\n",
    "        'mean_stopword_total': 'scaled_mean_stopword_total',\n",
    "        'mean_whitespace_count': 'scaled_mean_whitespace_count',\n",
    "        'mean_char_count': 'scaled_mean_char_count',\n",
    "        'mean_delim_count': 'scaled_mean_delim_count',\n",
    "        'stdev_stopword_total': 'scaled_stdev_stopword_total',\n",
    "        'stdev_whitespace_count': 'scaled_stdev_whitespace_count',\n",
    "        'stdev_char_count': 'scaled_stdev_char_count',\n",
    "        'stdev_delim_count': 'scaled_stdev_delim_count'\n",
    "    })\n",
    "\n",
    "    def abs_limit(x):\n",
    "        if abs(x) > 10000:\n",
    "            return 10000*np.sign(x)\n",
    "        return x\n",
    "\n",
    "    data1['scaled_mean'] = data1['scaled_mean'].apply(abs_limit)\n",
    "    data1['scaled_std_dev'] = data1['scaled_std_dev'].apply(abs_limit)\n",
    "    data1['scaled_min'] = data1['scaled_min'].apply(abs_limit)    \n",
    "    data1['scaled_max'] = data1['scaled_max'].apply(abs_limit)\n",
    "    data1['total_vals'] = data1['total_vals'].apply(abs_limit)\n",
    "    data1['num_nans'] = data1['num_nans'].apply(abs_limit)    \n",
    "    data1['num_of_dist_val'] = data1['num_of_dist_val'].apply(abs_limit) \n",
    "    \n",
    "    column_names_to_normalize = [\n",
    "                                'total_vals',\n",
    "                                'num_nans',\n",
    "                                'num_of_dist_val',\n",
    "                                'scaled_mean','scaled_std_dev','scaled_min','scaled_max'\n",
    "                                ]\n",
    "    x = data1[column_names_to_normalize].values\n",
    "    x = np.nan_to_num(x)\n",
    "    x_scaled = StandardScaler().fit_transform(x)\n",
    "    df_temp = pd.DataFrame(\n",
    "        x_scaled, columns=column_names_to_normalize, index=data1.index)\n",
    "    data1[column_names_to_normalize] = df_temp\n",
    "\n",
    "    y.y_act = y.y_act.astype(float)\n",
    "    return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xtrain = pd.read_csv('../../Benchmark-Labeled-Data/data_train.csv')\n",
    "xtest = pd.read_csv('../../Benchmark-Labeled-Data/data_test.csv')\n",
    "\n",
    "# for i in range(0,1000,10):\n",
    "xtrain = xtrain.sample(frac=1,random_state=100).reset_index(drop=True)\n",
    "# print(len(xtrain))\n",
    "\n",
    "atr_train = xtrain.loc[:,['Attribute_name']]\n",
    "atr_test = xtest.loc[:,['Attribute_name']]\n",
    "# print(atr_train)\n",
    "\n",
    "samp_train = xtrain.loc[:,['sample_1']]\n",
    "samp_test = xtest.loc[:,['sample_1']]\n",
    "\n",
    "y_train = xtrain.loc[:,['y_act']]\n",
    "y_test = xtest.loc[:,['y_act']]\n",
    "\n",
    "\n",
    "dict_label = {\n",
    "    'numeric': 0,\n",
    "    'categorical': 1,\n",
    "    'datetime': 2,\n",
    "    'sentence': 3,\n",
    "    'url': 4,\n",
    "    'embedded-number': 5,\n",
    "    'list': 6,\n",
    "    'not-generalizable': 7,\n",
    "    'context-specific': 8\n",
    "}\n",
    "\n",
    "y_train['y_act'] = [dict_label[i] for i in y_train['y_act']]\n",
    "y_test['y_act'] = [dict_label[i] for i in y_test['y_act']]\n",
    "y_train\n",
    "\n",
    "\n",
    "X_train = func1(xtrain,y_train)\n",
    "X_test = func1(xtest,y_test)\n",
    "\n",
    "\n",
    "# X_train = func2(xtrain,xtrain1,1)\n",
    "# X_test = func2(xtest,xtest1,0)\n",
    "# print(atr_train)\n",
    "print(atr_train['Attribute_name'].values)\n",
    "\n",
    "X_train.reset_index(inplace=True,drop=True)\n",
    "y_train.reset_index(inplace=True,drop=True)\n",
    "X_test.reset_index(inplace=True,drop=True)\n",
    "y_test.reset_index(inplace=True,drop=True)\n",
    "# atr_train.reset_index(inplace=True,drop=True)\n",
    "# atr_test.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values\n",
    "\n",
    "\n",
    "# atr_train = atr_train.values\n",
    "# atr_test = atr_test.values\n",
    "\n",
    "\n",
    "structured_data_train = X_train \n",
    "structured_data_test = X_test\n",
    "\n",
    "\n",
    "list_sentences_train = atr_train['Attribute_name'].values\n",
    "list_sentences_test = atr_test['Attribute_name'].values\n",
    "\n",
    "list_sentences_train1 = samp_train['sample_1'].values\n",
    "list_sentences_test1 = samp_test['sample_1'].values\n",
    "\n",
    "\n",
    "print(list_sentences_train)\n",
    "\n",
    "for i in range(len(list_sentences_train)): list_sentences_train[i] = str(list_sentences_train[i])\n",
    "for i in range(len(list_sentences_test)): list_sentences_test[i] = str(list_sentences_test[i]) \n",
    "\n",
    "    \n",
    "for i in range(len(list_sentences_train1)): list_sentences_train1[i] = str(list_sentences_train1[i])\n",
    "for i in range(len(list_sentences_test1)): list_sentences_test1[i] = str(list_sentences_test1[i]) \n",
    "\n",
    "print(list_sentences_train)\n",
    "\n",
    "\n",
    "tokenizer = keras_text.Tokenizer(char_level = True)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "\n",
    "\n",
    "tokenizer1 = keras_text.Tokenizer(char_level = True)\n",
    "tokenizer1.fit_on_texts(list(list_sentences_train1))\n",
    "\n",
    "# train data\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "X_t = keras_seq.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "\n",
    "list_tokenized_train1 = tokenizer.texts_to_sequences(list_sentences_train1)\n",
    "X_t1 = keras_seq.pad_sequences(list_tokenized_train1, maxlen=maxlen)\n",
    "\n",
    "# test data\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_te = keras_seq.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "list_tokenized_test1 = tokenizer.texts_to_sequences(list_sentences_test1)\n",
    "X_te1 = keras_seq.pad_sequences(list_tokenized_test1, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Bidirectional\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D, AveragePooling1D\n",
    "\n",
    "\n",
    "def build_model(neurons, numfilters,embed_size):\n",
    "    inp = Input(shape=(None, ))\n",
    "    x = Embedding(input_dim = len(tokenizer.word_counts)+1, output_dim = embed_size)(inp)\n",
    "#     prefilt_x = Dropout(0.5)(x)\n",
    "    out_conv = []  \n",
    "    \n",
    "#     x = prefilt_x\n",
    "    for i in range(2):\n",
    "        x = Conv1D(numfilters, kernel_size = 3, activation = 'tanh', kernel_initializer='glorot_normal')(x)\n",
    "        numfilters = numfilters*2    \n",
    "\n",
    "#     out_conv += [Dropout(0.5)(GlobalMaxPool1D()(x))]\n",
    "    out_conv += [(GlobalMaxPool1D()(x))]  \n",
    "#     xy = Flatten()(out_conv)\n",
    "    out_conv += [GlobalMaxPool1D()(x)]\n",
    "    x += [GlobalMaxPool1D()(x)]\n",
    "    xy = concatenate(out_conv, axis = -1)  \n",
    "\n",
    "\n",
    "    inp1 = Input(shape=(None, ))\n",
    "    x = Embedding(input_dim = len(tokenizer.word_counts)+1, output_dim = embed_size)(inp1)\n",
    "    out_conv = []\n",
    "\n",
    "    for i in range(2):\n",
    "        x = Conv1D(numfilters, kernel_size = 3, activation = 'tanh', kernel_initializer='glorot_normal')(x)\n",
    "        numfilters = numfilters*2\n",
    "        \n",
    "    out_conv += [(GlobalMaxPool1D()(x))]\n",
    "    out_conv += [GlobalMaxPool1D()(x)]\n",
    "    x += [GlobalMaxPool1D()(x)]\n",
    "    xy1 = concatenate(out_conv, axis = -1)     \n",
    "    \n",
    "    \n",
    "    Str_input = Input(shape=(19,))\n",
    "    layersfin = keras.layers.concatenate([xy,xy1,Str_input])\n",
    "    x = BatchNormalization()(layersfin)\n",
    "#     x = Dense(1000, activation='tanh',kernel_initializer='glorot_uniform')(Str_input)\n",
    "\n",
    "    x = Dense(neurons, activation='tanh')(x)\n",
    "    x = Dropout(0.5)(x)    \n",
    "#     x = Dense(500, activation='tanh')(x)\n",
    "#     x = Dense(neurons, activation='relu')(x)\n",
    "    x = Dense(neurons, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)    \n",
    "#     x = Dense(1000, activation='relu',kernel_initializer='random_normal')(x)    \n",
    "#     x = Dense(1000, activation='tanh')(x)\n",
    "    x = Dense(9, activation='softmax')(x)\n",
    "    model = Model(inputs=[inp,inp1,Str_input], outputs=[x])\n",
    "    opt = keras.optimizers.Adam(learning_rate=3e-3)\n",
    "    opt = keras.optimizers.RMSprop(learning_rate=1e-2)    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model(100,100,100)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_t)\n",
    "print(X_t.shape)\n",
    "# print(y_train[1851:])\n",
    "print(len(y_train))\n",
    "print(structured_data_train)\n",
    "\n",
    "y_train = y_train.values\n",
    "structured_data_train = structured_data_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 25\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k)\n",
    "\n",
    "neurons = [100,500,1000]\n",
    "n_filters_grid = [32,64,128]\n",
    "embed_size = [64,128,256]\n",
    "\n",
    "\n",
    "models = []\n",
    "\n",
    "avgsc_lst,avgsc_val_lst,avgsc_train_lst = [],[],[]\n",
    "avgsc,avgsc_val,avgsc_train = 0,0,0\n",
    "i=0\n",
    "for train_index, test_index in kf.split(X_t):\n",
    "#     if i==1: break\n",
    "    file_path= 'CNN_best_model'+str(i)+'.h5'\n",
    "\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "    callbacks_list = [checkpoint] #early\n",
    "        \n",
    "#     print('\\n')\n",
    "    X_train_cur, X_test_cur = X_t[train_index], X_t[test_index]\n",
    "    X_train_cur1, X_test_cur1 = X_t1[train_index], X_t1[test_index]    \n",
    "    y_train_cur, y_test_cur = y_train[train_index], y_train[test_index]\n",
    "    structured_data_train_cur, structured_data_test_cur = structured_data_train[train_index],structured_data_train[test_index]\n",
    "    \n",
    "    X_train_train, X_val,y_train_train,y_val = train_test_split(X_train_cur,y_train_cur, test_size=0.25,random_state=100)\n",
    "    structured_data_train_train,structured_data_val = train_test_split(structured_data_train_cur, test_size=0.25,random_state=100)\n",
    "\n",
    "    \n",
    "    bestscore = 0\n",
    "    for neuro in neurons:\n",
    "        for ne in n_filters_grid:\n",
    "            for md in embed_size:\n",
    "                print('\\n-------------\\n')\n",
    "                print('Neurons:'+str(neuro))                \n",
    "                print('Num Filters:'+str(ne) + '   '+ 'Embed Size:' + str(md))\n",
    "                clf = build_model(neuro,ne,md)\n",
    "                history = clf.fit([X_train_cur,X_train_cur1,structured_data_train_cur], to_categorical(y_train_cur),\n",
    "                        validation_data=([X_test_cur,X_test_cur1,structured_data_test_cur], to_categorical(y_test_cur)),\n",
    "                        batch_size=batch_size, epochs=epochs, shuffle = True, callbacks=callbacks_list) \n",
    "\n",
    "                bestPerformingModel = load_model('CNN_best_model'+str(i)+'.h5')            \n",
    "\n",
    "                loss, bscr_train = bestPerformingModel.evaluate([X_train_cur,X_train_cur1,structured_data_train_cur],to_categorical(y_train_cur))\n",
    "                print(loss, bscr_train)\n",
    "                loss, bscr_val = bestPerformingModel.evaluate([X_test_cur,X_test_cur1,structured_data_test_cur],to_categorical(y_test_cur))\n",
    "                print(loss, bscr_val)            \n",
    "                loss, bscr = bestPerformingModel.evaluate([X_te,X_te1,structured_data_test],to_categorical(y_test))\n",
    "                print(loss, bscr)            \n",
    "                print('\\n-------------\\n')\n",
    "\n",
    "    bestPerformingModel = load_model('CNN_best_model'+str(i)+'.h5')\n",
    "    \n",
    "    \n",
    "    loss, bscr_train = bestPerformingModel.evaluate([X_train_cur,X_train_cur1,structured_data_train_cur],to_categorical(y_train_cur))\n",
    "    print(loss, bscr_train)\n",
    "    loss, bscr_val = bestPerformingModel.evaluate([X_test_cur,X_test_cur1,structured_data_test_cur],to_categorical(y_test_cur))\n",
    "    print(loss, bscr_val)    \n",
    "    loss, bscr = bestPerformingModel.evaluate([X_te,X_te1,structured_data_test],to_categorical(y_test))\n",
    "    print(loss, bscr)\n",
    "    \n",
    "    models.append(clf)\n",
    "        \n",
    "    avgsc_train = avgsc_train + bscr_train\n",
    "    avgsc_val = avgsc_val + bscr_val\n",
    "    avgsc = avgsc + bscr\n",
    "    \n",
    "    avgsc_train_lst.append(bscr_train)\n",
    "    avgsc_val_lst.append(bscr_val)\n",
    "    avgsc_lst.append(bscr)\n",
    "    \n",
    "    print('The training accuracy is:')\n",
    "    print(bscr_train)\n",
    "    print('The validation accuracy is:')\n",
    "    print(bscr_val)    \n",
    "    print('The test accuracy is:')    \n",
    "    print(bscr)\n",
    "    print('\\n')\n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "avgsc_lst,avgsc_val_lst,avgsc_train_lst = [],[],[]\n",
    "\n",
    "i=0\n",
    "for train_index, test_index in kf.split(X_t):\n",
    "    X_train_cur, X_test_cur = X_t[train_index], X_t[test_index]\n",
    "    X_train_cur1, X_test_cur1 = X_t1[train_index], X_t1[test_index]    \n",
    "    y_train_cur, y_test_cur = y_train[train_index], y_train[test_index]\n",
    "    print(len(X_train_cur),len(X_test_cur))\n",
    "    print(len(y_train_cur), len(y_test_cur))\n",
    "    structured_data_train_cur, structured_data_test_cur = structured_data_train[train_index],structured_data_train[test_index]\n",
    "#     print(len(structured_data_train_cur),len(structured_data_test_cur))\n",
    "    print(len(X_te),len(y_test))\n",
    "    \n",
    "    bestPerformingModel = load_model('CNN_best_model'+str(i)+'.h5')\n",
    "\n",
    "    loss, bscr_train = bestPerformingModel.evaluate([X_train_cur,X_train_cur1,structured_data_train_cur],to_categorical(y_train_cur))\n",
    "    print(loss, bscr_train)\n",
    "    loss, bscr_val = bestPerformingModel.evaluate([X_test_cur,X_test_cur1,structured_data_test_cur],to_categorical(y_test_cur))\n",
    "    print(loss, bscr_val)    \n",
    "    loss, bscr = bestPerformingModel.evaluate([X_te,X_te1,structured_data_test],to_categorical(y_test))\n",
    "    print(loss, bscr)\n",
    "    \n",
    "    avgsc_train_lst.append(bscr_train)\n",
    "    avgsc_val_lst.append(bscr_val)\n",
    "    avgsc_lst.append(bscr)\n",
    "    print('\\n')\n",
    "    i=i+1\n",
    "print(avgsc_train_lst)\n",
    "print(avgsc_val_lst)\n",
    "print(avgsc_lst)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(avgsc_train_lst)\n",
    "print(avgsc_val_lst)\n",
    "print(avgsc_lst)\n",
    "print(np.mean(avgsc_train_lst))\n",
    "print(np.mean(avgsc_val_lst))\n",
    "print(np.mean(avgsc_lst))\n",
    "\n",
    "y_pred = bestPerformingModel.predict([X_te,X_te1,structured_data_test])\n",
    "y_pred1 = [np.argmax(i) for i in y_pred]\n",
    "cm = confusion_matrix(y_test,y_pred1)\n",
    "print('Confusion Matrix: Actual (Row) vs Predicted (Column)')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
